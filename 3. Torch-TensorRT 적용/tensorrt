1.	Torch-TensorRT 적용 후 inference 속도 차이 비교

-	python에서 제공하는 time 라이브러리는 CPU에서 측정되기 때문에 정확한 측정이 어 려워 pytorch에서 제공하는 torch.cuda.Event()를 이용했다.
-	Torch-TensorRT 적용 전 속도는 0.1426ms인데, 적용 후 속도는 0.0952ms로으로 이전 보다 약 33% 감소하였다.
-	결론: Tensor-TorchRT는 효율적인 하드웨어 가속 및 모델 최적화를 제공하여 모델의 추론 속도를 향상시킴과 동시에 모델의 메모리 사용량도 줄일 수 있다.

[Torch-TensorRT 적용 전]
Total inference time for the test set: 0.14262seconds

[Torch-TensorRT 적용 후]
Total inference time for the test set: 0.09527seconds

